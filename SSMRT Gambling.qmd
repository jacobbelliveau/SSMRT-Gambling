---
title: "SSMRT Gambling"
date: '`r Sys.Date()`'
toc: TRUE
embed-resources: true
output-location: column-fragment
---

# Screening, Self-Management, and Referral to Treatment - Gambling Iteration

Data Management code for the pilot survey of the Gambling Iteration of the Screening, Self-Management, and Referral to Treatment project (SSMRT-G).

This code accomplishes a few things:

-   Fetches data files (either locally or directly from REDCap)

-   Verifies participant IP addresses, compared against self-reported location

-   Various data quality checks

    -   Long-string analysis (also known as straight-lining)

    -   Speeding

    -   Inconsistency/attention checks

-   Publishes recruitment statistics

More detail for each of these steps can be found in their corresponding sections.

```{r packages}
library(REDCapR)
library(openxlsx)
library(stringr)
library(rlang)
library(dplyr)
library(lubridate)
library(knitr)
library(tidyverse)
library(table1)
library(googlesheets4)
library(quarto)

source("keys.R")


```

```{r data}

if (exists("REDCAP_URI") & exists("REDCAP_TOKEN")) {
  
  df <- redcap_read_oneshot(REDCAP_URI, REDCAP_TOKEN)$data
  rm(REDCAP_TOKEN, REDCAP_URI)
  
  RAW_DATA <- df
  
}

```

# IP filtering

```{r ipfunc}

ipdat <- function(ip_addresses, token) {
  # Checking for the HTTR package
  if (!require("httr", character.only = TRUE, quietly = TRUE)) {
    stop("Required package 'httr' is not installed. Please install before using this function.")
  }
  # Checking for the jsonlite package
  if (!require("jsonlite", character.only = TRUE, quietly = TRUE)) {
    stop("Required package 'jsonlite' is not installed. Please install before using this function.")
  }
  
  # Validating token
  validateToken <- httr::GET(paste0("https://ipinfo.io?token=", token))
  
  # If token is invalid, stop
  if (httr::status_code(validateToken) != 200) {
    httr::status_code(validateToken)
    stop("Invalid API token, or problems with the service.")
  }
  
  # Generating endpoint for API
  endpoint <- paste0("https://ipinfo.io/batch?token=", token)
  
  # converting IP address list to JSON
  payload <- jsonlite::toJSON(ip_addresses)
  
  # Getting IP data
  response <- httr::POST(endpoint, body = payload, content_type("application/json"))
  
  # Selecting and returning only content; stops if the response failed
  if (httr::http_type(response) == "application/json") {
    data <- httr::content(response, as = "text")
    parsed_data <- jsonlite::fromJSON(data)
    
    return(parsed_data)
  } else {
    stop("API request failed")
  }
}


# Function for extracting info from a list of lists
# Complement to the above function
# Note that returned columns are coerced into character types
unlistLists <- function(listNest, data_points) {
  # Extract the relevant data points from each IP info entry
  extracted_data <- lapply(listNest, function(entry) {
    extracted_entry <- lapply(data_points, function(point) {
      if (point %in% names(entry)) {
        entry[[point]]  # Extract the data point from the entry if it exists
      } else {
        NA_character_  # Fill missing values with NA
      }
    })
    extracted_entry <- as.list(extracted_entry)
    if (any(!is.na(unlist(extracted_entry)))) extracted_entry else NULL  # Filter out empty entries
  })
  
  # Filter out empty entries
  non_empty_entries <- extracted_data[lengths(extracted_data) > 0]
  
  if (length(non_empty_entries) > 0) {
    # Combine non-empty entries into a data frame and convert list columns to separate columns
    df <- as.data.frame(do.call(rbind, non_empty_entries), stringsAsFactors = FALSE)
    df <- as.data.frame(lapply(df, as.character), stringsAsFactors = FALSE)
    colnames(df) <- data_points
    rownames(df) <- seq_len(nrow(df))
  } else {
    # Create an empty data frame with appropriate column names
    df <- data.frame(matrix(ncol = length(data_points)))
    colnames(df) <- data_points
  }
  
  return(df)
}

```

```{r ip}

IPS <- read.xlsx("SSMRT Gambling Participant Info.xlsx", sheet = "IP")
LOCATIONS <- read.xlsx("SSMRT Gambling Participant Info.xlsx", sheet = "Location")


# seeing which IPs in the IP file have no corresponding location data in the location file
unfetched_IPs <- IPS$record_id[!(IPS$record_id %in% LOCATIONS$id)]

if (!is_empty(unfetched_IPs)) { # if there is at least 1 IP address
  for (id in unfetched_IPs) { # for each id
    ip <- IPS$ip[IPS$record_id == id] # get the corresponding IP address
    loc <- unlistLists( # (returned as a list so this unlists)
      ipdat(ip, IPINFO_API_TOKEN), # fetch location data
      c("region")) # keeping only the region column
    loc$id <- id # adding ip address to the region
    LOCATIONS <- rbind(LOCATIONS, loc) # binding this row of data to the larger location data file
  }
  rm(loc,ip,id) # removing unneeded data used in this loop
}

# combining the location data into the larger dataset, joining by record ID
df <- left_join(
  df, LOCATIONS, by = c("record_id" = "id")
)

rm(IPINFO_API_TOKEN, unfetched_IPs, ipdat, unlistLists)

```

# Speeding

```{r speeding}

### Detecting speeding
# creating a column with the time difference between the start time and end time
df$completetime <- difftime(df$exittime, df$starttime)

# calculating a cutoff (1/3 median of the above)
cutoff <- median(df$completetime[!is.na(df$completetime)]) * 0.3
cutoff # displaying cutoff

df$speeder <- 0 # creating a "speeder" column initialized to 0
df$speeder[df$completetime <= cutoff] <- 1 # setting to 1 for those where the complete time is less or equal to than the median

# removing unneeded environment objects
rm(cutoff)


```

# Inconsistency checks

```{r tensincon}

df$incon_tens <- 0
df$incon_tens[df$attncheck_tens2 %in% c(1,2,4,5)] <- 1

```

```{r iosincon}

df$incon_ios <- 0
df$incon_ios[df$gambling_screen != df$attncheck_ios] <- 1
df$incon_ios[is.na(df$attncheck_ios)] <- 0

```

```{r gmqincon}

df$incon_gmq <- 0
df$incon_gmq[df$attncheck_gmq %in% c(1,3,4)] <- 1

```

```{r provincon}

df$province_label <- dplyr::recode(df$province, 
                        '1'	= 'Alberta',
                        '2'	= 'British Columbia',
                        '3'	= 'Manitoba',
                        '4'	= 'New Brunswick',
                        '5'	= 'Newfoundland and Labrador',
                        '6'	= 'Nova Scotia',
                        '7'	= 'Ontario',
                        '8'	= 'Prince Edward Island',
                        '9'	= 'Quebec',
                        '10'	= 'Saskatchewan',
                        '11'	= 'Northwest Territories',
                        '12'	= 'Nunavut',
                        '13'	= 'Yukon',
                        '14' = "None of the above - I live elsewhere"
                                   )

df$incon_province <- 0
df$incon_province[df$province_label != df$region] <- 1
df$incon_province[is.na(df$province) | is.na(df$region)] <- 0

```

```{r ageincon}

df$incon_age <- 0
df$incon_age[df$age != df$age_screen] <- 1
df$incon_age[is.na(df$age)] <- 0

```

# Straight-lining

```{r longstringfunc}

SL.check <- function(df, vars, new_col, ignore.na = TRUE, ignore.value = c(), ignore.vars = c()) {
  # Check if vars are in the data frame
  if (!all(vars %in% colnames(df))) {
    stop("Error: one or more of the specified variables is not in the data frame.")
  }
  
  # finding the first and last indices of the vars variable
  firstCol <- which(colnames(df)==vars[1])
  lastCol <- which(colnames(df)==tail(vars,1))
  
  # getting the names of columns between both indices
  namesofcols <- names(df)[firstCol:lastCol]
  
  # if ignore.vars is not empty, remove those names from the namesofcols variable
  if (length(ignore.vars > 0)) {
    namesofcols <- namesofcols[!(namesofcols %in% ignore.vars)]
  }
  # Check if the variables have the same value
  if (ignore.na) {
    # Ignore missing values if ignore.na is TRUE
    same_value <- !apply(df[,namesofcols], 1, function(x) any(is.na(x)))
  } else {
    # Don't ignore missing values if ignore.na is FALSE
    same_value <- !apply(df[,namesofcols], 1, function(x) any(is.na(x)))
  }
  
  # If ignore.value is not empty, check if all values are NOT in ignore.value
  if (length(ignore.value) > 0) {
    same_value <- same_value & !apply(df[,namesofcols], 1, function(x) any(x %in% ignore.value))
  }
  
  # Check if all pairs of variables are equal
  same_value <- same_value & apply(df[,namesofcols], 1, function(x) all(x == x[1]))
  
  # Add the same_value vector as a new column in the data frame
  df[,new_col] <- same_value
  
  # Return the modified data frame
  df
}

```

```{r longstring}

df <- SL.check(df, c("gmq1", "gmq15"), "SL_gmq", ignore.vars = c("attncheck_gmq"))
df <- SL.check(df, c("pbss_1", "pbss_16"), "SL_pbss")
df <- SL.check(df, c("pgsi_1", "pgsi_9"), "SL_pgsi")

# Add a new column called "SL.total" to the data frame
first_SL <- which(colnames(df) == "SL_gmq")
last_SL <- which(colnames(df) == "SL_pgsi")

df$SL_total <- rowSums(df[first_SL:last_SL])

# Creates a flag which is 0 if they pass, and 1 if participants were flagged
# for straightlining more than 2 times.
df$SL_flag <- 0
df$SL_flag[df$SL_total > 2] <- 1

# removing unneeded environment objects
rm(SL.check, first_SL, last_SL)

```

# Manual exclusions

```{r manual}

MANUAL <- read.xlsx("SSMRT Gambling Participant Info.xlsx", sheet = "Manual decisions")

df$manual <- 0
df$manual[df$record_id %in% MANUAL$record_id] <- 1

```

# Exclusion summary

```{r exclude}

df$exclude <- 0
df$exclude[
    df$speeder == 1
  | df$incon_tens == 1
  | df$incon_gmq == 1
  | df$incon_ios == 1
  | df$incon_age == 1
  | df$incon_province == 1
  | df$SL_flag == 1
  | df$include_data == 0
  | df$province != 6
  # catching those who DQ or otherwise don't start the survey
  | is.na(df$starttime) | !is.na(df$dqtime)
  # catching those who didn't make it to the end
  | is.na(df$exittime)
  | df$manual == 1
  | df$withdraw_data___1 == 1
] <- 1

# excluding for quotas
df$quota_exclude <- df$exclude
df$quota_exclude[
    is.na(df$sr) |
    df$gender %in% c(3,4,5)
] <- 1

# removing those who withdrew
df$exclude[
  df$withdraw_data___1 == 1
] <- 1
```

# Scale scoring

Calculates summary scores for scales in the dataset.

```{r scores}

## PNCQ
#pncq_vars <- names(select(df, matches("pncq_\\d")))

#df$pncq_total <- rowSums(select(df, all_of(pncq_vars)), na.rm = FALSE)

## pgsi
pgsi_vars <- names(select(df, matches("pgsi_\\d")))

df$pgsi_total <- rowSums(select(df, all_of(pgsi_vars)), na.rm = FALSE)

## GMQ
gmq_vars <- names(select(df, matches("gmq\\d")))

gmq_enh <- gmq_vars[c(3,6,9,12,15)]
gmq_soc <- gmq_vars[c(1,4,7,10,13)]
gmq_cop <- gmq_vars[c(2,5,8,11,14)]

df$gmq_enh_total <- rowSums(select(df, all_of(gmq_enh)), na.rm = FALSE)
df$gmq_soc_total <- rowSums(select(df, all_of(gmq_soc)), na.rm = FALSE)
df$gmq_cop_total <- rowSums(select(df, all_of(gmq_cop)), na.rm = FALSE)

## PBSS
pbss_vars <- names(select(df, matches("pbss_\\d")))

df$pbss_total <- rowSums(select(df, all_of(pbss_vars)), na.rm = FALSE)

rm(gmq_cop, gmq_soc, gmq_enh, gmq_vars, pbss_vars, pgsi_vars)

```

# Google Analytics

Fetches data from the Google Analytics API.

```{r ga_data}

GA_DATA <- read.csv("SSMRT GA Data.csv")

if (exists("GA_TOKEN")) {
  library(googleAnalyticsR)
  library(googleAuthR)
  library(dplyr)
  
  # note to future self/selves: before the code below will work, you will need to authenticate manually first. i used ga_auth_setup using json files that i fetch from the GA website.
  
  # authenticating
  ga_auth(token = GA_TOKEN, email = GA_EMAIL)
  
  #meta <- ga_meta("data", propertyId = GA_TOKEN)

  # formatting starttime to POSIX
  df$ga_starttime <- strptime(df$starttime, format = "%Y-%m-%d %H:%M:%S")
  # removing the time, leaving only date
  df$ga_starttime <- round_date(df$ga_starttime, "day")
  
  # finding missing tokens from the DF
  ga_missing <- df$generated_token[!(df$generated_token %in% GA_DATA$generated_token) & df$exclude == 0 & !is.na(df$generated_token)]
  
  # appending those to GA_DATA
  GA_DATA <- rbind(GA_DATA, data.frame(generated_token = ga_missing, 
                                       first_visit = NA, 
                                       page_view = NA, 
                                       screen_view = NA, 
                                       session_start = NA, 
                                       user_engagement = NA, 
                                       UserEngagementDuration = NA))

  for (TOKEN in ga_missing) {
    filter <- ga_data_filter("customUser:current_user"==TOKEN)
    # first day boundary
    D1 <- df$ga_starttime[df$generated_token == TOKEN & !is.na(df$generated_token)] - ddays(1)
    # second day boundary
    D2 <- D1 + ddays(2)
    
    dat <- try(ga_data(
      propertyId = 330608169,
      metrics = c("eventCount", "userEngagementDuration"),
      date_range = c(D1, D2),
      dimensions = c("customUser:current_user", "eventName"),
      dim_filters = filter,
      limit = -1
    ))
    
    if (exists("dat") & all(class(dat) != "try-error")) {
      duration <- sum(as.numeric(dat$userEngagementDuration))
      GA_DATA$UserEngagementDuration[GA_DATA$generated_token == TOKEN] <- duration

      if ("first_visit" %in% dat$eventName) {
        FV <- dat$eventCount[dat$eventName == "first_visit"]
        GA_DATA$first_visit[GA_DATA$generated_token == TOKEN] <- FV}

      if ("page_view" %in% dat$eventName) {
        PV <- dat$eventCount[dat$eventName == "page_view"]
        GA_DATA$page_view[GA_DATA$generated_token == TOKEN] <- PV}

      if ("screen_view" %in% dat$eventName) {
        SV <- dat$eventCount[dat$eventName == "screen_view"]
        GA_DATA$screen_view[GA_DATA$generated_token == TOKEN] <- SV}

      if ("session_start" %in% dat$eventName) {
        SS <- dat$eventCount[dat$eventName == "session_start"]
        GA_DATA$session_start[GA_DATA$generated_token == TOKEN] <- SS}

      if ("user_engagement" %in% dat$eventName) {
        UE <- dat$eventCount[dat$eventName == "user_engagement"]
        GA_DATA$user_engagement[GA_DATA$generated_token == TOKEN] <- UE}

      suppressWarnings(rm(FV, PV, SS, SV, UE, duration, GA_TOKEN, D1, D2, filter, TOKEN))
    }
  }
}

df$ga_starttime <- NULL
df <- left_join(df, GA_DATA, by = "generated_token")
write.csv(GA_DATA, "SSMRT GA Data.csv", row.names = FALSE)

```

# Quota tables

Creates and publishes quota tables to Google Sheets.

```{r table}

# for testing
# df$province <- round(runif(39, 1, 14))
# df$gender <- round(runif(39, 1, 2))

quota <- subset(df, quota_exclude == 0)

quota$gender_label <- dplyr::recode(quota$gender,
                                 '1' = "Male",
                                 '2' = "Female")

# nice html table
if (length(quota$record_id > 20)) {
  quota_table <- table1::table1(~ province_label | gender_label, quota, overall = FALSE)
}


# less nice table for google sheets
quota_table_gs <- quota %>%
     group_by(province_label, gender_label) %>%
     summarise(N = n()) %>%
     pivot_wider(names_from = gender_label, values_from = N, values_fill = list(N = 0))

quota_table_gs <- rename(quota_table_gs, Province = province_label)

```

```{r reason}

EXCLUSION_REASONS <- subset(df, select = c(record_id, sr, gender, dqtime, exittime, starttime, withdraw_data___1, region, speeder:quota_exclude))

EXCLUSION_REASONS$reason <- "Not excluded"
EXCLUSION_REASONS$reason[EXCLUSION_REASONS$exclude == 1] <- "Failed quality checks"
EXCLUSION_REASONS$reason[is.na(EXCLUSION_REASONS$sr)] <- "Missing tracking number"
EXCLUSION_REASONS$reason[EXCLUSION_REASONS$gender %in% c(3,4,5)] <- "Failed quality checks"
EXCLUSION_REASONS$reason[is.na(EXCLUSION_REASONS$starttime) | 
                           is.na(EXCLUSION_REASONS$exittime)] <- "DNF"
EXCLUSION_REASONS$reason[EXCLUSION_REASONS$withdraw_data___1 == 1] <- "Withdrawal request"
EXCLUSION_REASONS$reason[!is.na(EXCLUSION_REASONS$dqtime)] <- "Screenout"
EXCLUSION_REASONS$reason[EXCLUSION_REASONS$region != "Nova Scotia" & 
                           !is.na(EXCLUSION_REASONS$region)] <- "Not Nova Scotian"

EXCLUSION_REASONS <- subset(EXCLUSION_REASONS, select = c(record_id, sr, reason))

EXCLUSION_REASONS_TABLE <- pivot_wider(summarise(group_by(EXCLUSION_REASONS, reason), N = n()), names_from = reason, values_from = N, values_fill = list(N = 0))

```

```{r gsheets}

if (exists("USER_EMAIL") & exists("DOC_ID")) {
  
   gs4_auth(email = USER_EMAIL)
  
   sheet_write(quota_table_gs,
               ss = DOC_ID,
               sheet = "N")
  
}

```

# Saving to file

```{r save}

## Saving location data
write.xlsx(list("IP" = IPS,
                "Location" = LOCATIONS,
                "Manual decisions" = MANUAL,
                "Quota exclusion reasons" = EXCLUSION_REASONS,
                "Quota exclusions summary" = EXCLUSION_REASONS_TABLE
                ), "SSMRT Gambling Participant Info.xlsx", 
           rowNames = FALSE)

## Output
# Checks for the existence of a folder in the project directory named "output" - if it does not exist, creates it.
if (!dir.exists("output")) {dir.create("output")}

# Creates the folder name for today's output
todays_folder_name <- paste("output", Sys.Date(), "", sep = "/")

# if the above folder doesn't exist, creates it
if (!dir.exists(todays_folder_name)) {dir.create(todays_folder_name)}

# writing raw data to file
write.csv(RAW_DATA, paste(todays_folder_name, "SSMRT rawData.csv", sep = ""), row.names = FALSE)

# writing cleaned data to file (including those excluded)
write.csv(df, paste(todays_folder_name, "SSMRT dataAll.csv", sep = ""), row.names = FALSE)

# writing cleaned data to file (excluding those excluded)
write.csv(subset(df, exclude == 0), paste(todays_folder_name, "SSMRT dataFinal.csv", sep = ""), row.names = FALSE)

```
